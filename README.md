ğŸ“˜ RAG-Based News Digest Application

A lightweight, fast, and fully local Retrieval-Augmented Generation app built with Streamlit, FAISS, GGUF models, and a complete ingestion â†’ preprocessing â†’ embedding â†’ indexing pipeline.

ğŸš€ Overview

This application generates daily news digests using a complete RAG (Retrieval-Augmented Generation) workflow.
It ingests online news, cleans and chunks text, embeds it, stores embeddings in a FAISS vector store, and answers queries using a quantized GGUF LLM running on CPU.

The app is designed to be:

Fast (GGUF quantized models, FAISS search)

Fully local (no API calls)

Cloud deployable (Hugging Face Spaces, AWS EC2, GCP, Azure)

Modular (each pipeline step as a separate script)

Cost-efficient (runs on CPU)





news-digest-rag/
â”‚
â”œâ”€â”€ app.py                     â† Streamlit UI (entrypoint)
â”œâ”€â”€ rag_query.py               â† Will be modified for HF caching + relative paths
â”‚
â”œâ”€â”€ models/                    â† Quantized model + tokenizer
â”‚   â”œâ”€â”€ mistral-7b-instruct-v0.2.Q4_K_M.gguf
â”‚   â”œâ”€â”€ tokenizer.model        â† (if needed)
â”‚
â”œâ”€â”€ index/                     â† FAISS index + metadata (generated locally)
â”‚   â”œâ”€â”€ news_index.index
â”‚   â”œâ”€â”€ news_index.pkl
â”‚
â”œâ”€â”€ pipeline/                  â† Local-only RAG pipeline steps
â”‚   â”œâ”€â”€ ingest_news.py
â”‚   â”œâ”€â”€ preprocess_and_chunk.py
â”‚   â”œâ”€â”€ embed_chunks.py
â”‚   â”œâ”€â”€ store_faiss.py
â”‚   â””â”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ requirements.txt           â† HF-friendly deps
â”œâ”€â”€ README.md                  â† Instructions
â””â”€â”€ .gitignore


ğŸš€ Overview

This project builds a full News RAG (Retrieval-Augmented Generation) system:

Ingests real news from multiple RSS feeds

Preprocesses and chunks the text

Embeds all chunks using Sentence Transformers

Stores vectors in FAISS for high-speed retrieval

Runs a GGUF quantized LLM locally via llama-cpp-python

Serves results in a clean Streamlit UI

ğŸ”„ RAG Pipeline (Local Only)

All pipeline files are inside:

pipeline/

Steps:

Fetch news

ingest_news.py


Clean + chunk articles

preprocess_and_chunk.py


Generate embeddings

embed_chunks.py


Store FAISS index + metadata

store_faiss.py


Run full pipeline

run_pipeline.py


âš ï¸ This pipeline MUST be run locally. Hugging Face Spaces cannot run it due to time & memory limits.

It generates:

index/news_index.index
index/news_index.pkl


These should be committed to Git so they load instantly on Hugging Face.

ğŸ§  Models Used

Stored in:

models/


Includes:

mistral-7b-instruct-v0.2.Q4_K_M.gguf (quantized for CPU-fast inference)

tokenizer.model (optional depending on the GGUF file)

Inference is handled by:

llama-cpp-python


No GPU is required.

ğŸ›ï¸ Running Locally
1. Install dependencies
pip install -r requirements.txt

2. Build the FAISS index (first-time only)
python pipeline/run_pipeline.py

3. Run the Streamlit application
streamlit run app.py


The app loads:

The GGUF model from /models

The FAISS index from /index

All metadata from /index/news_index.pkl

ğŸŒ Deploying to Hugging Face Spaces

This project is optimized for Spaces (Streamlit runtime).

Steps

Commit the whole folder to GitHub

Create a Hugging Face Space â†’ select Streamlit

Connect repository â†’ Spaces auto-builds and runs

Ensure your requirements.txt contains:

streamlit
llama-cpp-python
faiss-cpu
sentence-transformers
numpy
requests
feedparser
newspaper3k
torch


(Optional extras removed for faster build.)

During inference:

The GGUF model loads from /models

The FAISS index loads from /index

No pipeline scripts run on Spaces

This keeps startup fast and avoids HF RAM/time limits.

ğŸ“¦ What Runs on Hugging Face

âœ” app.py
âœ” rag_query.py
âœ” Model loading (GGUF)
âœ” FAISS search
âœ” Generation with llama-cpp

âŒ News ingestion
âŒ Chunking
âŒ Embedding
âŒ FAISS index creation

Pipeline must run locally and only outputs are pushed to Git.

ğŸ§¹ .gitignore Essentials

Included so we avoid committing:

news_digest_venv/
__pycache__/
*.pyc
*.DS_Store

ğŸ¤ Contributing

Feel free to open issues or PRs for improvements:

More news sources

Better chunking logic

Faster embedding/model options

UI enhancements

ğŸ“„ License

MIT License